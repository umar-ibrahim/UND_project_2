{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wragle_report\n",
    "* Create a **300-600 word written report** called \"wrangle_report.pdf\" or \"wrangle_report.html\" that briefly describes your wrangling efforts. This is to be framed as an internal document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle Report \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"gather.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This report is about my second project **Data Wrangling** on Twitter account **@dog_rates** also known as **WeRateDogs**. The process is in three stages of **gathering, assessing and cleaning data.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering (Stage 1)\n",
    "Gathering is the **first stage** in any data wranging process that takes us form having no dataset to having a dataset. Without gathering data, we have nothing to analyse or play with.\n",
    "#### Datasets: \n",
    "- Twitter_archive_enhanced.csv\n",
    "- Image_prediction.tsv\n",
    "- Tweet_json.txt\n",
    "### imports:\n",
    "- Imported pandas, numpy,matplotlib\n",
    "\n",
    "#### twitter_archive_enhanced.csv\n",
    "- Directly downloaded  and uploaded the **twitter_archive_enhanced.csv** dataset\n",
    "- inserted it into a pandas datafram called **df_twitter_archive** using pd.read_csv\n",
    "\n",
    "#### image_prediction\n",
    "- image_prediction is a url\n",
    "- Stored the url in a variable called **url**, used **response.get** method to get the response (200 means success)\n",
    "- Opened a file and saved the content (file name: img.tsv)\n",
    "- Inserted it into a pandas datafram called **df_img** using pd.read_csv.\n",
    "\n",
    "#### Additional data via tweet_json.txt. \n",
    "- I couldn't yet get twitter API keys, So,\n",
    "- I used save as to save the **tweet_json.txt**, created an  empty list and a new file, and looped through line by line, putting it in a data variable and appended it to our empty **tweet_list** using append.\n",
    "- pd.DataFrame was used to insert the list to a dataframe called **tweet_json**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing (Stage 2)\n",
    "Asseesing is the **second stage** after data gathering and the procedure is made up of\n",
    "**Visual** and **programmatic assessment**. Result of both was detected and documented under **quality and tidiness**\n",
    "#### Visual Assessment (manually viewing/scrolling dataframe by typing the datafram name)\n",
    "- df_twitter_archive\n",
    "- df_img \n",
    "- df_tweet_json \n",
    "\n",
    "#### Programmatic assessment was carreid out using the following\n",
    " \n",
    "- dataframe.head()\n",
    "- dataframe.info() \n",
    "- dataframe.describe()\n",
    "- dataframe.sample()\n",
    "- dataframe.value_counts()\n",
    "\n",
    "**Note: df_twitter_archive,df_img, df_tweet_json were used in place of dataframe above. e.g df_img.describe() **\n",
    "\n",
    "### Quality and Tidiniess issues were detected using both assessments\n",
    "#### Summary of detected issues \n",
    "##### Quality:\n",
    "- 1. Timestamp is object and should be datetime\n",
    "- 2. in_reply_to_status_id has only 78 non-null values\n",
    "- 3. in_reply_to_user_id has only 78 non-null values\n",
    "- 4. retweeted_status_id has only 181 non-null values\n",
    "- 5. retweeted_status_timestamp has only 181 non-null values\n",
    "- 6. retweeted_status_user_id has only 181 non-null values\n",
    "- 7. source has anchor tags along with content and that needs to be extracted \n",
    "- 8. Not all images/predictions are dogs (False on p1_dog, p2_dog,p3_dog)\n",
    "\n",
    "#### Tidiness:\n",
    "- 1. columns doggo, floofer, pupper and puppo disobey Tidiness rule and should be under one column say dog_stages\n",
    "- 2. columns above should be dropped from the twitter archive enhanced dataset\n",
    "- 3. columns in_reply_to_status_id,in_reply_to_user_id,retweeted_status_id,retweeted_status_user_id should be dropped\n",
    "- 4. p1,p1_conf,p1_dog, p2,p2_conf,p2_dog\t,p3,p3_conf,p3_dog disobey tidiness rule and should be in two colums, breed and prediction\n",
    "- 5. columns p1,p1_conf,p1_dog, p2,p2_conf,p2_dog\t,p3,p3_conf,p3_dog should be droppped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning (Stage 3)\n",
    "This is the stage where all the issues detected and documented at the previous (assessing stage) were cleaned. I followed the template of defining, coding and testing which served as a road map and champions reproducibility. I detected 12 issues (Quality and Tidiness) which were cleaned. Columsn were dropped, ids that had reference to other tables were deleted,Timstamp was changed to datetime, regex was used to extract source of data from anchor tags and so on.I merged the three cleaned datasets into one using merge function and produced a master_archive.csv file. This file was saved and was used for the insights and visualization. \n",
    "\n",
    "Cleaning was divided (For each issue detected ) into:\n",
    "- Define: Defines the issue detected at the previous (Assessing stage)\n",
    "- Code: Code implementation to fix the defined issue\n",
    "- Test: Testing to see if code fixed the defined issue and a result obtained\n",
    "\n",
    "\n",
    "**Note**: Comments were also added for reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The above captures my wrangling efforts and it follows the process being taught in my Udacity classroom."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
